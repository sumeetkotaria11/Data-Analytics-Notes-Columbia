{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Trees and Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Decision trees are tree structures containing rules\n",
    "<li>The leaf nodes of the tree are the \"learned\" categories (or threshold values)\n",
    "<li>A path from the root to a leaf node represents a rule (or a decision path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example: A decision tree with rules on deciding who survived or died on the titanic</h3>\n",
    "<i>Source: https://en.wikipedia.org/wiki/Decision_tree_learning#/media/File:CART_tree_titanic_survivors.png</i>\n",
    "<li>To use the tree, enter a person-data object and you'll get an answer</li>\n",
    "<li>Ex: (\"John Brown\",\"Male\",\"30 years old\", \"3 siblings\") Ans: Survived (89% probability)\n",
    "<li>Ex: (\"Jessica Jones\", \"Female\",\"7 years old\", \"no siblings\") Ans: Survived (73% probability)\n",
    "<li>Ex: (\"Hercules Mulligan\", \"Male\", \"2 years old\", \"20 siblings\") Ans: Died (17% probability)\n",
    "<li>Note that the 17% probability doesn't mean that there is an 83% chance that Mulligan survived!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Class+11+-+CART_tree_titanic_survivors.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why decision trees?</h2>\n",
    "<li>Easy to understand \n",
    "<li>Rule finding process is transparent\n",
    "<li>Can handle \"mixed\" categorical(male/female) and numerical (age, number of siblings) data\n",
    "<li>Can handle missing data \n",
    "<li>Can be used to generate partial \"good\" solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why not decision trees?</h2>\n",
    "<li>Finding an optimal tree is a hard problem\n",
    "<li>Overfitting is a HUGE problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Types of decision trees</h2>\n",
    "<ul>\n",
    "<li><b>Classification trees</b>: Uses rules to classify cases into two or more categories (Rocks vs Mines)\n",
    "<ul>\n",
    "<li>Classification trees recursively split the data on a feature value\n",
    "<li>Each split minimizes the entropy (also known as the impurity)\n",
    "<li>Entropy is commonly measured using the GINI cost function (a measure of the probability of misclassification or 'purity')\n",
    "</ul>\n",
    "<li><b>Regression trees</b>: Uses rules to group data into target variable ranges (Wine Quality)\n",
    "<ul>\n",
    "<li>Also split the data on feature values\n",
    "<li>Minimize cost (impurity). Usually the mean squared error\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stopping and Pruning Rules</h3>\n",
    "<li>A minimum count of observations in each leaf node\n",
    "<li>A maximum tree <b>depth</b>\n",
    "<li>A maximum <b>complexity</b> (the number of splits)\n",
    "<li>Using all three, you won't necessarily have a balanced tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predicting wine quality using a decision tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "w_df = pd.read_csv(url,header=0,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Examining the dependent variable</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df['quality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Higher dv values indicate a better quality wine\n",
    "<li>Lower dv values indicate a poorer quality wine\n",
    "<li>We'll assume that the values are continuous\n",
    "<li>And use the various features to predict wine quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Build train and test samples</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(w_df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:11]\n",
    "y_train = train[['quality']]\n",
    "x_test = test.iloc[0:,0:11]\n",
    "y_test = test[['quality']]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classifiers vs Regressors</h3>\n",
    "<li>Decision tree regressors are used when the target variable is continuous and ordered (wine quality from 0 to 10)\n",
    "<li>Classifiers are used when the target variable is a set of unordered categories (rocks or mines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For wine quality, we need a regressor</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "#model = DecisionTreeRegressor(max_depth = 3)\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeRegressor(max_depth=3)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the R-Square for the predicted vs actuals on the text sample\n",
    "print(\"Training R-Square\",model.score(x_train,y_train))\n",
    "print(\"Testing R-Square\",model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>View the tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Download and install <a href=\"http://www.graphviz.org/download/\">graphviz</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having issues using Graphviz in Windows, then try the following steps:\n",
    "<ol>\n",
    "<li>1. Install Graphviz \n",
    "<li>2. After installing graphviz, add it to the Computer's Path. \n",
    "<ul>\n",
    "<li>Go to PC properties \n",
    "<li> Click environment variables in the advanced settings section\n",
    "<li> Add C:\\Program Files (x86)\\Graphviz2.38\\bin\\ to the PATH and click Apply\n",
    "</ul>\n",
    "<li> Install Pydotplus. <b>Note that you will always have to install pydot after graphviz as Pydot is Graphviz's dot language and needs Graphviz for reference</b>. \n",
    "</ol>\n",
    "<h3>Install pydotplus (using pip): Install graphviz before you install pydotplus!</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydotplus --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pydotplus \n",
    "feature_names = [key for key in w_df if not key=='quality']\n",
    "from IPython.display import Image\n",
    "dot_data = tree.export_graphviz(model, out_file=None,feature_names=feature_names)\n",
    "import pydotplus\n",
    "\n",
    "graph = pydotplus.graphviz.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision trees are Entropy minimizers</h3>\n",
    "<li><b>Entropy</b>: a measure of uncertainty in the data<p>\n",
    "<ul>\n",
    "<li>what is the uncertainty in color when you draw a marble from a box of 100 blue marbles?\n",
    "<li>what is the uncertainty when you draw a marble from a box with 50 blue and 50 red marbles?\n",
    "</ul>\n",
    "<li>Entropy minimization: decision tree algorithms seek to partition the data on features in the way that total entropy is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The danger of entropy minimization</h2>\n",
    "<li>In the degenerate case, we can build rules that partition the data into single case subsets\n",
    "<li>The resulting combined entropy will be zero!\n",
    "<li>But the results will be useless because we will likely not be able to predict anything if we get a new case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example</h2>\n",
    "<li>Feature set = {SSN,GENDER,GRE}\n",
    "<li>DV = {GPA}\n",
    "<li>Output rules:\n",
    "<ul>\n",
    "<li>if SSN = x1 then 3.2\n",
    "<li>if SSN = x2 then 4.2\n",
    "<li>One rule for each SSN in our training dataset\n",
    "</ul>\n",
    "<li>This will be totally useless in predicting what a the gpa of a new student is likely to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regression trees</h3>\n",
    "<li>Run regressions for each X to the dependent variable\n",
    "<li>Pick the variable with the most explanatory power and split it at several points\n",
    "<li>Calculate the Mean Square Error of each of the two halves for each split\n",
    "<li>Pick the split point that gives the lowest mse (combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification trees</h1>\n",
    "<h2>Classification trees are used when dealing with categorical dependent variables</h2>\n",
    "<li>Pick a variable and a split point so that the misclassification cost is the lowest.\n",
    "\n",
    "<h3>Rocks and mines data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df[60]=np.where(df[60]=='R',0,1)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:60]\n",
    "y_train = train[[60]]\n",
    "x_test = test.iloc[0:,0:60]\n",
    "y_test = test[[60]]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier(max_depth = 3,criterion='entropy')\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "feature_names = [key for key in df if not key == 60]\n",
    "dot_data = tree.export_graphviz(model, out_file=None,feature_names=feature_names) \n",
    "\n",
    "graph = pydotplus.graphviz.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[tn, fp]\n",
    "#  [fn, tp]]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p_train=model.predict(x_train)\n",
    "p_test = model.predict(x_test)\n",
    "print(confusion_matrix(p_train,np.array(y_train)))\n",
    "print(confusion_matrix(p_test,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "(fpr, tpr, thresholds) = roc_curve(y_test,p_test)\n",
    "area = auc(fpr,tpr)\n",
    "pl.clf() #Clear the current figure\n",
    "pl.plot(fpr,tpr,label=\"Out-Sample ROC Curve with area = %1.2f\"%area)\n",
    "\n",
    "pl.plot([0, 1], [0, 1], 'k') #This plots the random (equal probability line)\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('In sample ROC rocks versus mines')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Random forests</h1>\n",
    "<li>Build many decision trees from the data set\n",
    "<li>Let them \"vote\" on how to classify inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ensemble learning random forests</h2>\n",
    "<li>Use a random subset of features and choose the feature to split on from this subset\n",
    "<li>Repeat the process, this gives multiple different trees (the ensemble)\n",
    "<li>The model then predicts y values by letting the trees vote \n",
    "<ul>\n",
    "<li>The forest is given a case\n",
    "<li>Each tree decides which class the case belongs to\n",
    "<li>Votes are tallied\n",
    "<li>The highest vote wins\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "#np.ravel flattens the pandas Series into an np array. That's what the classifier needs\n",
    "model.fit(x_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>\n",
    "<li>The \"score\" function returns the accuracy of the model (percentage correctly classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(x_test)\n",
    "cfm = confusion_matrix(np.ravel(y_test),y_pred)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature importance</h2>\n",
    "<li>Since ensemble methods are picking different features in different trees, they can provide us with an estimate of feature importance\n",
    "<li>For each feature, the model calculates by how much entropy decreases (net across levels) by selecting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "importances = model.feature_importances_\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.barh(range(len(indices)), importances, color='b', align='center')\n",
    "dummy = plt.yticks(range(len(indices)),feature_names)\n",
    "# Sorted\n",
    "#indices = np.argsort(importances)\n",
    "#plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "#dummy = plt.yticks(range(len(indices)),indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding the best ensemble</h3>\n",
    "<li>Using a gridsearch, we can run the random forest classifier on various parameter combinations\n",
    "<li>And then use the classifier with the best accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'n_estimators':(10, 30, 50), #the number of trees\n",
    "     'max_depth':(4,5,6,8,10,15),\n",
    "     'min_samples_split': (2, 4, 8),\n",
    "     'min_samples_leaf': (4,8,12,16)\n",
    "}\n",
    "\n",
    "model = GridSearchCV(RandomForestClassifier(),parameters,cv=3,iid=False)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "model.best_score_, model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_model = RandomForestClassifier(max_depth=15,min_samples_leaf=12,min_samples_split=4,n_estimators=50)\n",
    "b_model.fit(x_train,np.ravel(y_train))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = b_model.predict(x_test)\n",
    "cfm = confusion_matrix(np.ravel(y_test),y_pred)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bootstrapping / Bagging</h2>\n",
    "<li>Create a synthetic dataset by drawing sample cases \"with replacement\" from the training set\n",
    "<li>Run the decision tree algorithm on this dataset\n",
    "<li>Repeat on multiple such synthetic datasets\n",
    "<li>Let the many trees vote on the class for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "model=BaggingClassifier()\n",
    "model.fit(x_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'n_estimators':(30, 50), #the number of trees\n",
    "     'max_samples':(30,40,50),\n",
    "     'max_features':(5,10,20),  \n",
    "}\n",
    "\n",
    "model = GridSearchCV(BaggingClassifier(),parameters,cv=3,iid=False)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "model.best_score_, model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BaggingClassifier(max_features=5,max_samples=10,n_estimators=30)\n",
    "model.fit(x_train,np.ravel(y_train))\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why random forests?</h2>\n",
    "<li>Can deal with missing data\n",
    "<ul>\n",
    "<li>If a feature is missing from a case, that case is discarded when a feature is considered for inclusion in the tree\n",
    "<li>In ensemble learning, since we're working with subsets of the features, every feature will figure in some tree or the other (and will have a vote)\n",
    "</ul>\n",
    "<li>Useful when the data available is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why not random forests?</h2>\n",
    "<li>Danger of overfitting, especially when the sample is small or when the number of classes is small\n",
    "<li>Can't explain the results. The rules are opaque since many trees are voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
